{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention Sub-layer\n"
      ],
      "metadata": {
        "id": "s060u-EN_tC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 1. Input: 3 inputs, d_model = 4\n",
        "\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "x = np.array([\n",
        "    [1.0, 0.0, 1.0, 0.0], # Input 1\n",
        "    [0.0, 2.0, 0.0, 2.0], # Input 2\n",
        "    [1.0, 1.0, 1.0, 1.0], # Input 3    \n",
        "])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJmg9ilBr_io",
        "outputId": "9e165adb-b4cb-42bb-f93f-759a41e398df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 1. 0.]\n",
            " [0. 2. 0. 2.]\n",
            " [1. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 2. Initialize the Key, Query, Value Matrices\n",
        "\n",
        "key = np.array([\n",
        "    [1, 0, 1],\n",
        "    [1, 0, 0],\n",
        "    [0, 0, 1],\n",
        "    [0, 1, 1]\n",
        "])\n",
        "query = np.array([\n",
        "    [1, 0, 1],\n",
        "    [1, 0, 0],\n",
        "    [0, 0, 1],\n",
        "    [0, 1, 1]\n",
        "])\n",
        "values = np.array([\n",
        "    [1, 0, 1],\n",
        "    [1, 0, 0],\n",
        "    [0, 0, 1],\n",
        "    [0, 1, 1]\n",
        "])"
      ],
      "metadata": {
        "id": "0M801yiHsYAX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 3. Matrix multiplication to obtain Q, K, V\n",
        "\n",
        "K = np.matmul(x, key)\n",
        "Q = np.matmul(x, query)\n",
        "V = np.matmul(x, values)"
      ],
      "metadata": {
        "id": "x4GA4nO4svzX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 4. Scale attention scores\n",
        "\n",
        "# I had no idea the @ operator did matrix multiplication, that's crazy.\n",
        "attention_scores = (Q @ K.transpose()) / np.sqrt(len(x))\n",
        "# This attention scoreing it based on the origin transformer equation\n",
        "\n",
        "print(attention_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkTOIBCutbSw",
        "outputId": "3a9ec55a-23a7-4b0f-821d-9716300586d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.88675135 3.46410162 4.61880215]\n",
            " [3.46410162 6.92820323 6.92820323]\n",
            " [4.61880215 6.92820323 8.08290377]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 5. Scale Softmax Attention Scores\n",
        "\n",
        "for i, attention_score in enumerate(attention_scores):\n",
        "  attention_scores[i] = softmax(attention_score)\n",
        "\n",
        "print(attention_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "edt9kSCFuPEZ",
        "outputId": "7a761a12-522d-4b7f-8afd-50377db10065"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.11857409 0.21121746 0.67020845]\n",
            " [0.01540939 0.4922953  0.4922953 ]\n",
            " [0.02324709 0.23406082 0.74269209]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 6. Final Attention Representations\n",
        "\n",
        "attention_0_input_1 = attention_scores[0][0] * V[0]\n",
        "attention_1_input_1 = attention_scores[0][1] * V[1]\n",
        "attention_2_input_1 = attention_scores[0][2] * V[2]\n",
        "\n",
        "attention_0_input_2 = attention_scores[1][0] * V[0]\n",
        "attention_1_input_2 = attention_scores[1][1] * V[1]\n",
        "attention_2_input_2 = attention_scores[1][2] * V[2]\n",
        "\n",
        "attention_0_input_3 = attention_scores[2][0] * V[0]\n",
        "attention_1_input_3 = attention_scores[2][1] * V[1]\n",
        "attention_2_input_3 = attention_scores[2][2] * V[2]"
      ],
      "metadata": {
        "id": "FIYOHrUnuiPA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 7. Sum all attention representations for the input\n",
        "\n",
        "attention_input_1 = attention_0_input_1 + attention_1_input_1 + attention_2_input_1\n",
        "attention_input_2 = attention_0_input_2 + attention_1_input_2 + attention_2_input_2\n",
        "attention_input_3 = attention_0_input_3 + attention_1_input_3 + attention_2_input_3\n",
        "\n",
        "print(f'Attention Score for Input 1: {attention_input_1}')\n",
        "print(f'Attention Score for Input 2: {attention_input_2}')\n",
        "print(f'Attention Score for Input 3: {attention_input_3}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "VARRsUSCwFhw",
        "outputId": "26f6ac91-ad2c-4638-ddcc-02a7fe014f1a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Score for Input 1: [1.88142591 1.09264338 2.67020845]\n",
            "Attention Score for Input 2: [1.98459061 1.47688591 2.4922953 ]\n",
            "Attention Score for Input 3: [1.97675291 1.21081373 2.74269209]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 8. Concatenation of the output heads\n",
        "\n",
        "print('Let\\'s assume for a moment that steps 2-7 happened 8 times, for all 8 \\'heads\\' of the transformer')\n",
        "\n",
        "print('This means that we have effectively 8x each of the vectors above')\n",
        "\n",
        "print('We just concatenate them to obtain the output dimensions we require')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "JQBTaDowxcjv",
        "outputId": "c74ca755-e13b-41c4-a2f3-cd1b73b87b85"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's assume for a moment that steps 2-7 happened 8 times, for all 8 'heads' of the transformer\n",
            "This means that we have effectively 8x each of the vectors above\n",
            "We just concatenate them to obtain the output dimensions we require\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 9. Post-Layer Normalization Step\n",
        "\n",
        "print('This is just a normalization layer, nothing special here')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vvyH66VqyKM-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}