{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9IACeZ9CpGY"
      },
      "source": [
        "# Robustly Optimized BERT Pretraining Approach (RoBERTa)\n",
        "\n",
        "## Improvements over BERT\n",
        "\n",
        "### Byte-Pair Encoding (BPE)\n",
        "- Alternative to WordPiece tokenization (Word to embedding)\n",
        "- Break down words into subcomponents:\n",
        "    - Example: \"I like smaller cats\" could become [\"I\", \"like\", \"small\", \"er\", \"cats\"]\n",
        "- Pros:\n",
        "    - Allows the model understand words it hasn't seen before\n",
        "    - Target vocabulary can be smaller\n",
        "\n",
        "### DistilBERT\n",
        "- Smaller model than RoBERTa with much better performance\n",
        "- Results are slightly less accurate than RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agLD9vjUco97",
        "outputId": "45ca3fc1-7940-4689-e77f-7c325d4ec18e"
      },
      "outputs": [],
      "source": [
        "#@title I. Load Dataset and Setup GPU\n",
        "!curl -L https://raw.githubusercontent.com/Denis2054/Transformers-for-NLP-2nd-Edition/master/Chapter04/kant.txt --output \"kant.txt\"\n",
        "\n",
        "#@ Check Nvidia\n",
        "import torch\n",
        "!nvidia-smi\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy_uPZ8wdmDU",
        "outputId": "01168581-c688-4e59-8b13-2c95c1668f02"
      },
      "outputs": [],
      "source": [
        "#@title II. Training a Byte-Level Tokenizer\n",
        "\n",
        "#@markdown #### Benefits of Byte-Level Tokenization\n",
        "#@markdown ##### - Allows for smaller target vocabulary\n",
        "#@markdown ##### - Allows for use of OOV words, because it uses subcomponents of words\n",
        "#@markdown #### Hugging Face\n",
        "#@markdown ##### - Tokenization is saved as two files\n",
        "#@markdown ######--> 'merges.txt': Merged tokenized substrings\n",
        "#@markdown ######--> 'vocab.json': Indices of the tokenized substrings\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Get all paths to the dataset\n",
        "paths = [str(x) for x in Path('.').glob('**/*.txt')]\n",
        "\n",
        "# Train the tokenizer\n",
        "tokenizer.train(files=paths, vocab_size=52000, min_frequency=2, special_tokens=[\n",
        "    '<s>',\n",
        "    '<pad>',\n",
        "    '</s>',\n",
        "    '<unk>',\n",
        "    '<mask>',\n",
        "])\n",
        "\n",
        "token_dir = '/content/KantaiBERT'\n",
        "if not os.path.exists(token_dir):\n",
        "  os.makedirs(token_dir)\n",
        "tokenizer.save_model('KantaiBERT')\n",
        "\n",
        "# It's weird but in the byte level tokenization,\n",
        "# 'Ä ' means a whitespace character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfWtDChwfTRb",
        "outputId": "90a1965f-2a06-4e24-b0f9-1ca46138333e"
      },
      "outputs": [],
      "source": [
        "#@title III. Load the Tokenizer\n",
        "\n",
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    './KantaiBERT/vocab.json',\n",
        "    './KantaiBERT/merges.txt',\n",
        ")\n",
        "\n",
        "# Add start and end token to the sentences.\n",
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    ('</s>', tokenizer.token_to_id('</s>')),\n",
        "    ('<s>', tokenizer.token_to_id('<s>')),    \n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)\n",
        "tokenizer.encode(\"I like many animals, especially cats.\").tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Mxu9eGsikIP6"
      },
      "outputs": [],
      "source": [
        "#@title Define RoBERTa Tokenizer and Model\n",
        "\n",
        "from transformers import RobertaTokenizer\n",
        "from transformers import RobertaConfig\n",
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('./KantaiBERT', max_length=512)\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size = 52000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads = 12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")\n",
        "model = RobertaForMaskedLM(config=config)\n",
        "\n",
        "# Print summary of model\n",
        "# print(model)\n",
        "\n",
        "# Print model parameters\n",
        "# print(model.num_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmwRG56pkwTb",
        "outputId": "3d279502-f612-4d3d-b511-3bedaf28f9d5"
      },
      "outputs": [],
      "source": [
        "#@title Build Dataset and Collator\n",
        "\n",
        "from transformers import LineByLineTextDataset\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='./kant.txt',\n",
        "    block_size=128, # Batch size\n",
        ")\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUGchYVrmZsn",
        "outputId": "e0ebe837-fa06-4db2-a3e6-dcb83fc4c740"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OW2OrV7nlq3o"
      },
      "outputs": [],
      "source": [
        "#@title Initialize the Trainer\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = './KantaiBERT',\n",
        "    overwrite_output_dir = True,\n",
        "    num_train_epochs = 1,\n",
        "    per_device_train_batch_size = 64,\n",
        "    save_steps = 10000,\n",
        "    save_total_limit = 2,\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "GH-6co1GmGXH",
        "outputId": "158e8c9b-f559-45f7-e066-c49718f51de9"
      },
      "outputs": [],
      "source": [
        "#@title Pre-Train the Model\n",
        "%%time\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9WXvDHGzoEuM"
      },
      "outputs": [],
      "source": [
        "#@title Save Model (To Dir with Tokenizer and Config)\n",
        "trainer.save_model('./KantaiBERT')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tEw0QJSWqxtj"
      },
      "outputs": [],
      "source": [
        "#@title Perform Fill-Mask\n",
        "from transformers import pipeline\n",
        "fill_mask = pipeline(\n",
        "    'fill-mask',\n",
        "    model = './KantaiBERT',\n",
        "    tokenizer='./KantaiBERT',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Eu-bYXdwq_ex",
        "outputId": "6d5f0eb7-eab7-48db-8452-48831e5d2f07"
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "import random\n",
        "\n",
        "sentence = 'The reason for human existence has less to do with'\n",
        "\n",
        "while True:\n",
        "  masked = f'{sentence} <mask>'\n",
        "  values = fill_mask(masked)\n",
        "  val_idx = random.randint(0, len(last_value)-1)\n",
        "  last_value = values[val_idx]\n",
        "  print(last_value)\n",
        "  sentence += (last_value['token_str'])\n",
        "  print(sentence)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
